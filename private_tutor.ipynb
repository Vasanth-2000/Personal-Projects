{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# Core Python utilities\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from IPython.display import Markdown, display, update_display\n",
    "\n",
    "\n",
    "# Environment variable loading\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# OpenAI client\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "# set up environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Load OpenAI API key from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if OPENAI_API_KEY and OPENAI_API_KEY.startswith(\"sk-\") and len(OPENAI_API_KEY) > 10:\n",
    "    print(\"✅ OpenAI API key loaded successfully\")\n",
    "else:\n",
    "    print(\"⚠️ OpenAI API key missing or invalid. Check your .env file.\")\n",
    "\n",
    "# Initialise OpenAI client\n",
    "openai = OpenAI(api_key=OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3afc4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a friendly technical tutor.\n",
    "Explain the answer clearly in 3 parts:\n",
    "1) What it does\n",
    "2) Why it works\n",
    "3) Common pitfalls / edge cases\n",
    "Keep it concise but helpful. Use small examples if useful.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06869c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "def explain_with_openai_streaming(question_text: str):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question_text.strip()},\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    response_text = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta\n",
    "        if delta and getattr(delta, \"content\", None):\n",
    "            response_text += delta.content\n",
    "            update_display(Markdown(response_text), display_id=display_handle.display_id)\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "def explain_with_ollama(question_text: str, model: str = MODEL_LLAMA):\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question_text.strip()},\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    r = requests.post(\"http://localhost:11434/api/chat\", json=payload, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9bfd18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OpenAI (streaming) ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure! Let's break down the code you provided step-by-step:\n",
       "\n",
       "### 1) What it does\n",
       "This code uses a generator expression to yield each unique author's name from a collection of `books`. Specifically:\n",
       "- It goes through each `book` in the `books` iterable (which is presumably a list of dictionaries).\n",
       "- For each `book`, it tries to retrieve the value associated with the key `\"author\"` using `book.get(\"author\")`.\n",
       "- It includes the author in the result only if the author is not `None` or an empty string (due to the condition `if book.get(\"author\")`).\n",
       "- `yield from` causes the generator to yield each value produced by the included generator expression, which only gets unique authors due to the use of a set.\n",
       "\n",
       "### 2) Why it works\n",
       "- **Generator Expression**: `{book.get(\"author\") for book in books if book.get(\"author\")}` creates a set of authors, effectively filtering out duplicates and excluding any non-existent authors. The set comprehension evaluates to a set of author names.\n",
       "- **`yield from`**: This keyword allows values produced from the set (which includes unique authors) to be yielded one by one within a generator function. This means each time the generator is iterated over, it will return the next author until all have been processed.\n",
       "\n",
       "### 3) Common pitfalls / edge cases\n",
       "- **Empty or Missing \"author\" Keys**: If the `books` list is empty or if none of the books have the \"author\" key, the output will simply yield nothing.\n",
       "  \n",
       "   ```python\n",
       "   books = []\n",
       "   # No authors to yield, will produce no output.\n",
       "   ```\n",
       "   \n",
       "- **Non-string Author Values**: If `book.get(\"author\")` returns values that are not strings (e.g., `None`, numbers, or objects), these values may affect what gets included in the set.\n",
       "  \n",
       "   ```python\n",
       "   books = [{\"author\": None}, {\"author\": 42}, {\"author\": \"Author A\"}]\n",
       "   # Only \"Author A\" will be yielded.\n",
       "   ```\n",
       "\n",
       "- **Mutability of Set**: If for any reason the contents of `books` change while iterating, it may lead to unexpected results if the generator is reused. \n",
       "\n",
       "- **Performance**: If `books` contains a large number of entries, the performance could be affected due to set uniqueness checks, though this is typically not noticeable in most practical scenarios.\n",
       "\n",
       "In summary, this code efficiently filters and yields unique author names from a collection of books while taking care to exclude any missing or invalid entries!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== OpenAI (streaming) ===\")\n",
    "_ = explain_with_openai_streaming(question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d0cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n=== Ollama (llama3.2) ===\")\n",
    "ollama_answer = explain_with_ollama(question)\n",
    "display(Markdown(ollama_answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
